from itertools import product

WORKLOADS = [
  "a", "b", "c", "d", "e", "f",
]

DBS = [
  "pg_llsm",
  "rocksdb",
]

CONFIGS = [
  {
    "name": "64B",
    "record_size_bytes": 64,

    # Used by LLSM.
    "llsm_page_fill_pct": 70,

    # Used by page-grouped LLSM.
    "records_per_page_goal": 45,
    "records_per_page_delta": 5,

    # Used by all DBs.
    # 408 MiB in total (2 x 64 MiB + 280 MiB, ~33% of the dataset)
    "memtable_mib": 64,
    "cache_mib": 280,

    # Used to configure the length of the workload.
    "point_requests": 8000000,
    "scan_requests": 2400000,
  },
  {
    "name": "1024B",
    "record_size_bytes": 1024,

    # Used by LLSM.
    "llsm_page_fill_pct": 50,

    # Used by page-grouped LLSM.
    "records_per_page_goal": 2,
    "records_per_page_delta": 1,

    # Used by all DBs.
    # 3255 MiB in total ~33% of the dataset
    "memtable_mib": 510,
    "cache_mib": 2235,

    # Used to configure the length of the workload.
    # These request counts are 16x lower than the ones used for the 64B
    # workload.
    "point_requests": 500000,
    "scan_requests": 150000,
  },
]

COMMON_OPTIONS = {
  "bg_threads": 4,
  "latency_sample_period": 10,

  # Affects RocksDB & LLSM
  "bypass_wal": True,
  "use_direct_io": True,

  # Affects RocksDB
  "rdb_bloom_bits": 10,
  "rdb_prefix_bloom_size": 3,

  # Affects LLSM
  "reorg_length": 2,
  "use_alex": False,
  "deferral_autotuning": True,
  "max_deferrals": 1,
}

THREADS = [1, 2, 4, 8, 16]

SYNTH_DATASET = {
  "name": "synth",
  "mem_mult": 1,
  "range_min": 0,
  "range_max": 2000000000,
}

CUSTOM_DATASETS = [
  {
    "name": "osm",
    "path": "'$TP_DATASET_PATH/osm_ny.txt'",
    # The dataset is this many times larger than the standard 20 M synthetic
    # dataset we use.
    "mem_mult": 1.160804,
    "range_min": 699540,
    "range_max": 6820987374,
  },
  {
    "name": "amzn",
    "path": "'$TP_DATASET_PATH/amazon_reviews.txt'",
    # The dataset is this many times larger than the standard 20 M synthetic
    # dataset we use.
    "mem_mult": 1.674881,
    "range_min": 10001,
    "range_max": 53096592,
  },
]


###
### Utilities.
###

def process_config(db, config, dataset, workload=None):
  copy = config.copy()
  del copy["cache_mib"]
  del copy["memtable_mib"]
  del copy["name"]
  del copy["point_requests"]
  del copy["scan_requests"]

  # Set the memory configuration.
  if db == "pg_llsm":
    copy["cache_size_mib"] = int(dataset["mem_mult"] * (
      config["cache_mib"] + (2 * config["memtable_mib"])
    ))
  else:
    copy["cache_size_mib"] = int(dataset["mem_mult"] * config["cache_mib"])
    copy["memtable_size_mib"] = int(dataset["mem_mult"] * config["memtable_mib"])

  # Set the arguments used to customize the workload.
  if workload is None or workload != "e":
    copy["gen_num_requests"] = config["point_requests"]
  else:
    copy["gen_num_requests"] = config["scan_requests"]

  # Sets the key range for inserts.
  copy["gen_range_min"] = dataset["range_min"]
  copy["gen_range_max"] = dataset["range_max"]

  return copy


###
### Get aggregated results.
###

run_command(
  name="combine-all",
  run="python3 combine_raw.py",
  deps=[
    ":synth",
    ":osm",
    ":amzn",
  ],
)

run_command(
  name="combine-64B",
  run="python3 combine_raw.py",
  deps=[":64B"],
)

run_command(
  name="combine-1024B",
  run="python3 combine_raw.py",
  deps=[":1024B"],
)

run_command(
  name="combine-single",
  run="python3 combine_raw.py",
  deps=[":single"],
)


###
### YCSB.
###

# All YCSB experiments.
combine(
  name="ycsb",
  deps=[
    ":synth",
    ":osm",
    ":amzn",
  ],
)

# All 64B workloads.
combine(
  name="64B",
  deps=[
    ":{}-{}-64B-{}-{}".format(*zipped)
    for zipped in product(["synth", "osm", "amzn"], DBS, WORKLOADS, THREADS)
  ],
)

# All 1024B workloads.
combine(
  name="1024B",
  deps=[
    ":{}-{}-1024B-{}-{}".format(*zipped)
    for zipped in product(["synth", "osm", "amzn"], DBS, WORKLOADS, THREADS)
  ],
)

# All single-threaded workloads.
combine(
  name="single",
  deps=[
    ":{}-{}-{}-{}-1".format(*zipped)
    for zipped in product(["synth", "osm", "amzn"], DBS, ["64B", "1024B"], WORKLOADS)
  ],
)

run_experiment_group(
  name="synth",
  run="./run.sh",
  experiments=[
    # e.g. synth-pg_llsm-64B-a-1
    ExperimentInstance(
      name="synth-{}-{}-{}-{}".format(db, config["name"], workload, threads),
      options={
        **COMMON_OPTIONS,
        **process_config(db, config, SYNTH_DATASET, workload),
        "db": db,
        "checkpoint_name": "ycsb-synth-{}-{}".format(db, config["name"]),
        "threads": threads,
        "gen_template": "workloads/{}.yml".format(workload),
      },
    )
    for db, config, workload, threads in product(DBS, CONFIGS, WORKLOADS, THREADS)
  ],
  deps=[
    # e.g. :preload-synth-pg_llsm-64B
    ":preload-synth-{}-{}".format(db, config["name"])
    for db, config in product(DBS, CONFIGS)
  ],
)

for dataset in CUSTOM_DATASETS:
  # `amzn` and `osm`
  run_experiment_group(
    name="{}".format(dataset["name"]),
    run="./run.sh",
    experiments=[
      # e.g. amzn-pg_llsm-64B-a-1
      ExperimentInstance(
        name="{}-{}-{}-{}-{}".format(dataset["name"], db, config["name"], workload, threads),
        options={
          **COMMON_OPTIONS,
          **process_config(db, config, dataset, workload),
          "db": db,
          "checkpoint_name": "ycsb-{}-{}-{}".format(dataset["name"], db, config["name"]),
          "custom_dataset": dataset["path"],
          "threads": threads,
          "gen_template": "workloads/{}.yml".format(workload),
        },
      )
      for db, config, workload, threads in product(DBS, CONFIGS, WORKLOADS, THREADS)
    ],
    deps=[
      # e.g. :preload-amzn-pg_llsm-64B
      ":preload-{}-{}-{}".format(dataset["name"], db, config["name"])
      for db, config in product(DBS, CONFIGS)
    ],
  )


###
### Preload.
###

# Run all preload tasks.
group(
  name="preload",
  deps=[
    ":preload-{}-{}-{}".format(*zipped)
    for zipped in product(
      ["synth", "osm", "amzn"],
      DBS,
      map(lambda c: c["name"], CONFIGS),
    )
  ],
)

for db, config in product(DBS, CONFIGS):
  run_command(
    name="preload-synth-{}-{}".format(db, config["name"]),
    run="./preload.sh",
    options={
      **COMMON_OPTIONS,
      **process_config(db, config, SYNTH_DATASET),
      "db": db,
      "checkpoint_name": "ycsb-synth-{}-{}".format(db, config["name"]),
      "threads": 1,
      "gen_template": "workloads/setup.yml",
    },
  )

  for dataset in CUSTOM_DATASETS:
    run_command(
      name="preload-{}-{}-{}".format(dataset["name"], db, config["name"]),
      run="./preload.sh" ,
      options={
        **COMMON_OPTIONS,
        **process_config(db, config, dataset),
        "db": db,
        "checkpoint_name": "ycsb-{}-{}-{}".format(dataset["name"], db, config["name"]),
        "custom_dataset": dataset["path"],
        "threads": 1,
        "gen_template": "workloads/setup.yml",
      },
    )
