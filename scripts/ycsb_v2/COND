from itertools import product

WORKLOADS = [
  "a", "b", "c", "d", "e", "f",
]

DBS = [
  "pg_llsm",
  "rocksdb",
  "leanstore"
]

DISTRIBUTIONS = ["zipfian", "uniform"]

THREADS = [1, 2, 4, 8, 16]

CONFIG_64B = {
  "name": "64B",
  "record_size_bytes": 64,

  # Used by LLSM.
  "tl_page_fill_pct": 70,

  # Used by page-grouped LLSM.
  "records_per_page_goal": 44,
  "records_per_page_epsilon": 5,

  # Used by all DBs.
  # 408 MiB in total (2 x 64 MiB + 280 MiB, ~33% of the dataset)
  "memtable_mib": 64,
  "cache_mib": 280,

  # Used to configure the length of the workload.
  # These values are scaled up by the dataset size multiplier.
  "point_requests": 15000000,  # 15 million
  "scan_requests":  1000000,   # 1 million

  "rec_cache_use_lru": False,
}

CONFIG_1024B = {
  "name": "1024B",
  "record_size_bytes": 1024,

  # Used by LLSM.
  "tl_page_fill_pct": 50,

  # Used by page-grouped LLSM.
  "records_per_page_goal": 2,
  "records_per_page_epsilon": 0.5,

  # Used by all DBs.
  # 6510 MiB in total ~33% of the dataset
  "memtable_mib": 1024,
  "cache_mib": 4462,

  # Used to configure the length of the workload.
  # These values are scaled up by the dataset size multiplier.
  "point_requests": 7500000,  # 7.5 million
  "scan_requests": 500000,    # 500 thousand
}

CONFIGS = [CONFIG_64B, CONFIG_1024B]

COMMON_OPTIONS = {
  "bg_threads": 4,
  "latency_sample_period": 10,

  # Affects RocksDB & LLSM
  "bypass_wal": True,
  "use_direct_io": True,

  # Affects RocksDB
  "rdb_bloom_bits": 10,
  "rdb_prefix_bloom_size": 3,

  # Affects LLSM
  "reorg_length": 2,
  "deferral_autotuning": True,
  "max_deferrals": 1,

  # Affects LLSM and page-grouped LLSM
  "optimistic_rec_caching": False,

  # Affects page-grouped LLSM
  "pg_use_pgm_builder": True,
}

SYNTH_DATASET = {
  "name": "synth",
  "multiplier": 1,
  "range_min": 1,
  "range_max": 2000000000,
}

OSM_DATASET = {
  "name": "osm",
  "path": "'$TP_DATASET_PATH/osm_ny.txt'",
  # The dataset is this many times larger than the standard 20 M synthetic
  # dataset we use.
  "multiplier": 1.160804,
  "range_min": 699540,
  "range_max": 6820987374,
}

AMZN_DATASET = {
  "name": "amzn",
  "path": "'$TP_DATASET_PATH/amazon_reviews.txt'",
  # The dataset is this many times larger than the standard 20 M synthetic
  # dataset we use.
  "multiplier": 1.674881,
  "range_min": 10001,
  "range_max": 53096592,
}

CUSTOM_DATASETS = [OSM_DATASET, AMZN_DATASET]

# Number of uniform updates to make during the preload process. The purpose of
# these updates is to ensure that RocksDB has sstables present in multiple
# levels.
NUM_PRELOAD_UPDATES = 40000000


###
### Utilities.
###

def process_config(db, config, dataset, workload=None):
  copy = config.copy()
  del copy["cache_mib"]
  del copy["memtable_mib"]
  del copy["name"]
  del copy["point_requests"]
  del copy["scan_requests"]

  # Set the memory configuration.
  if db == "pg_llsm":
    copy["cache_size_mib"] = int(dataset["multiplier"] * (
      config["cache_mib"] + (2 * config["memtable_mib"])
    ))
  else:
    copy["cache_size_mib"] = int(dataset["multiplier"] * config["cache_mib"])
    copy["memtable_size_mib"] = int(dataset["multiplier"] * config["memtable_mib"])

  # Set the arguments used to customize the workload.
  if workload is not None and (workload == "e" or workload == "scan_only"):
    copy["gen_num_requests"] = int(dataset["multiplier"] * config["scan_requests"])
  else:
    copy["gen_num_requests"] = int(dataset["multiplier"] * config["point_requests"])

  # Sets the key range for inserts.
  copy["gen_range_min"] = dataset["range_min"]
  copy["gen_range_max"] = dataset["range_max"]

  # Enable parallel final flush when the workload does not contain inserts.
  # The workload is set to None when running a preload, which only has updates.
  if workload is None or (workload != "d" and workload != "e"):
    copy["pg_parallelize_final_flush"] = True

  return copy


###
### Get aggregated results.
###

run_command(
  name="combine-all",
  run="python3 combine_raw.py",
  deps=[
    ":synth",
    ":osm",
    ":amzn",
  ],
)

run_command(
  name="combine-main",
  run="python3 combine_raw.py",
  deps=[":main"],
)

run_command(
  name="combine-main-zipfian",
  run="python3 combine_raw.py",
  deps=[":main-zipfian"],
)

###
### YCSB.
###

ALL_EXPERIMENTS = [
  ":{}-{}-{}-{}-{}-{}".format(*zipped)
  for zipped in product(
    ["synth", "osm", "amzn"],
    DBS,
    ["64B", "1024B"],
    WORKLOADS,
    DISTRIBUTIONS,
    THREADS,
  )
  # The uniform and zipfian "d" workloads are the same, so just run one.
  if not (zipped[3] == "d" and zipped[4] == "uniform")
]

# "Main" tasks are the ones we prioritize.

MAIN = [
  exp_name
  for exp_name in ALL_EXPERIMENTS
  if ("amzn" in exp_name) or ("-e-" in exp_name)
]

MAIN_ZIPFIAN = [
  exp_name for exp_name in MAIN if "zipfian" in exp_name
]

MAIN_PGLLSM = [
  exp_name for exp_name in MAIN if "pg_llsm" in exp_name
]

MAIN_PGLLSM_ZIPFIAN = [
  exp_name for exp_name in MAIN_PGLLSM if "zipfian" in exp_name
]

# All YCSB experiments.
combine(
  name="ycsb",
  deps=[
    ":synth",
    ":osm",
    ":amzn",
  ],
)

combine(name="main", deps=MAIN)
combine(name="main-zipfian", deps=MAIN_ZIPFIAN)
combine(name="main-pg_llsm", deps=MAIN_PGLLSM)
combine(name="main-pg_llsm-zipfian", deps=MAIN_PGLLSM_ZIPFIAN)

combine(
  name="main-ycsb-e",
  deps=[
    exp_name for exp_name in MAIN if "-e-" in exp_name
  ],
)

# The tasks below are used for dividing up the experiments across 2 machines.

group(
  name="ycsb-1-of-2",
  deps=[
    exp_name for idx, exp_name in enumerate(ALL_EXPERIMENTS) if idx % 2 == 0
  ],
)

group(
  name="ycsb-2-of-2",
  deps=[
    exp_name for idx, exp_name in enumerate(ALL_EXPERIMENTS) if idx % 2 == 1
  ],
)

group(
  name="main-1-of-2",
  deps=[
    exp_name for idx, exp_name in enumerate(MAIN) if idx % 2 == 0
  ],
)

group(
  name="main_zipfian-1-of-2",
  deps=[
    exp_name for idx, exp_name in enumerate(MAIN_ZIPFIAN) if idx % 2 == 0
  ],
)

group(
  name="main_pg_llsm-1-of-2",
  deps=[
    exp_name for idx, exp_name in enumerate(MAIN_PGLLSM) if idx % 2 == 0
  ],
)

group(
  name="main_pg_llsm_zipfian-1-of-2",
  deps=[
    exp_name for idx, exp_name in enumerate(MAIN_PGLLSM_ZIPFIAN) if idx % 2 == 0
  ],
)

group(
  name="main-2-of-2",
  deps=[
    exp_name for idx, exp_name in enumerate(MAIN) if idx % 2 == 1
  ],
)

group(
  name="main_zipfian-2-of-2",
  deps=[
    exp_name for idx, exp_name in enumerate(MAIN_ZIPFIAN) if idx % 2 == 1
  ],
)

group(
  name="main_pg_llsm-2-of-2",
  deps=[
    exp_name for idx, exp_name in enumerate(MAIN_PGLLSM) if idx % 2 == 1
  ],
)

group(
  name="main_pg_llsm_zipfian-2-of-2",
  deps=[
    exp_name for idx, exp_name in enumerate(MAIN_PGLLSM_ZIPFIAN) if idx % 2 == 1
  ],
)

###
### The actual YCSB experiment definitions.
###

run_experiment_group(
  name="synth",
  run="./run.sh",
  experiments=[
    # e.g. synth-pg_llsm-64B-a-zipfian-1
    ExperimentInstance(
      name="synth-{}-{}-{}-{}-{}".format(db, config["name"], workload, dist, threads),
      options={
        **COMMON_OPTIONS,
        **process_config(db, config, SYNTH_DATASET, workload=workload),
        "db": db,
        "checkpoint_name": "ycsb-synth-{}-{}".format(db, config["name"]),
        "threads": threads,
        "gen_template": "workloads/{}.yml".format(workload),
        "gen_distribution": dist,
      },
    )
    for db, config, workload, dist, threads in product(
      DBS,
      CONFIGS,
      WORKLOADS,
      DISTRIBUTIONS,
      THREADS,
    )
    # The uniform and zipfian "d" workloads are the same, so just run one.
    if not (workload == "d" and dist == "uniform")
  ],
  deps=[
    # e.g. :preload-synth-pg_llsm-64B
    ":preload-synth-{}-{}".format(db, config["name"])
    for db, config in product(DBS, CONFIGS)
  ],
)

for dataset in CUSTOM_DATASETS:
  # `amzn` and `osm`
  run_experiment_group(
    name="{}".format(dataset["name"]),
    run="./run.sh",
    experiments=[
      # e.g. amzn-pg_llsm-64B-a-zipfian-1
      ExperimentInstance(
        name="{}-{}-{}-{}-{}-{}".format(dataset["name"], db, config["name"], workload, dist, threads),
        options={
          **COMMON_OPTIONS,
          **process_config(db, config, dataset, workload=workload),
          "db": db,
          "checkpoint_name": "ycsb-{}-{}-{}".format(dataset["name"], db, config["name"]),
          "custom_dataset": dataset["path"],
          "threads": threads,
          "gen_template": "workloads/{}.yml".format(workload),
          "gen_distribution": dist,
        },
      )
      for db, config, workload, dist, threads in product(
        DBS,
        CONFIGS,
        WORKLOADS,
        DISTRIBUTIONS,
        THREADS,
      )
      # The uniform and zipfian "d" workloads are the same, so just run one.
      if not (workload == "d" and dist == "uniform")
    ],
    deps=[
      # e.g. :preload-amzn-pg_llsm-64B
      ":preload-{}-{}-{}".format(dataset["name"], db, config["name"])
      for db, config in product(DBS, CONFIGS)
    ],
  )


###
### Factor analysis (depends on two YCSB experiments).
###

run_command(
  name="combine-factor",
  run="python3 combine_raw.py",
  args=["--for-factor"],
  deps=[":factor"]
)

combine(
  name="factor",
  deps=[
    # No page grouping, no caching.
    ":factor-nogrp_nocache-{}-16".format(wkload)
    for wkload in WORKLOADS
  ] + [
    # No page grouping, yes caching.
    ":factor-nogrp-{}-16".format(wkload)
    for wkload in WORKLOADS
  ] + [
    # Yes page grouping, yes caching.
    ":amzn-pg_llsm-1024B-{}-zipfian-16".format(wkload)
    for wkload in WORKLOADS
  ],
)

for wkload in WORKLOADS:
  run_experiment(
    name="factor-nogrp_nocache-{}-16".format(wkload),
    run="./run.sh",
    options={
      **COMMON_OPTIONS,
      **process_config("pg_llsm", CONFIG_1024B, AMZN_DATASET, workload=wkload),
      "db": "pg_llsm",
      "checkpoint_name": "ycsb-amzn-pg_llsm_nogrp-1024B",
      "custom_dataset": AMZN_DATASET["path"],
      "threads": 16,
      "gen_template": "workloads/{}.yml".format(wkload),
      "gen_distribution": "zipfian",
      # Disable the cache and page grouping
      "pg_bypass_cache": True,
      "pg_use_segments": False,
    },
    deps=[":preload-amzn-pg_llsm_nogrp-1024B"],
  ) 

for wkload in WORKLOADS:
  run_experiment(
    name="factor-nogrp-{}-16".format(wkload),
    run="./run.sh",
    options={
      **COMMON_OPTIONS,
      **process_config("pg_llsm", CONFIG_1024B, AMZN_DATASET, workload=wkload),
      "db": "pg_llsm",
      "checkpoint_name": "ycsb-amzn-pg_llsm_nogrp-1024B",
      "custom_dataset": AMZN_DATASET["path"],
      "threads": 16,
      "gen_template": "workloads/{}.yml".format(wkload),
      "gen_distribution": "zipfian",
      # Enable the cache, disable page grouping
      "pg_bypass_cache": False,
      "pg_use_segments": False,
    },
    deps=[":preload-amzn-pg_llsm_nogrp-1024B"],
  )

###
### Preload.
###

# Run all preload tasks.
group(
  name="preload",
  deps=[
    ":preload-{}-{}-{}".format(*zipped)
    for zipped in product(
      ["synth", "osm", "amzn"],
      DBS,
      map(lambda c: c["name"], CONFIGS),
    )
  ],
)

for db, config in product(DBS, CONFIGS):
  run_command(
    name="preload-synth-{}-{}".format(db, config["name"]),
    run="./preload.sh",
    options={
      **COMMON_OPTIONS,
      **process_config(db, config, SYNTH_DATASET),
      "db": db,
      "checkpoint_name": "ycsb-synth-{}-{}".format(db, config["name"]),
      "threads": 1,
      "gen_template": "workloads/setup.yml",
      # NOTE: This overrides any previously set value.
      "gen_num_requests": NUM_PRELOAD_UPDATES,
    },
  )

  for dataset in CUSTOM_DATASETS:
    run_command(
      name="preload-{}-{}-{}".format(dataset["name"], db, config["name"]),
      run="./preload.sh" ,
      options={
        **COMMON_OPTIONS,
        **process_config(db, config, dataset),
        "db": db,
        "checkpoint_name": "ycsb-{}-{}-{}".format(dataset["name"], db, config["name"]),
        "custom_dataset": dataset["path"],
        "threads": 1,
        "gen_template": "workloads/setup.yml",
        # NOTE: This overrides any previously set value.
        "gen_num_requests": NUM_PRELOAD_UPDATES,
      },
    )

# This is used for the factor analysis experiment.
run_command(
  name="preload-amzn-pg_llsm_nogrp-1024B",
  run="./preload.sh" ,
  options={
    **COMMON_OPTIONS,
    **process_config("pg_llsm", CONFIG_1024B, AMZN_DATASET),
    "db": "pg_llsm",
    "checkpoint_name": "ycsb-amzn-pg_llsm_nogrp-1024B",
    "custom_dataset": AMZN_DATASET["path"],
    "threads": 1,
    "gen_template": "workloads/setup.yml",
    # NOTE: This overrides any previously set value.
    "gen_num_requests": NUM_PRELOAD_UPDATES,
    # IMPORTANT: Disable page grouping (factor analysis)
    "pg_use_segments": False,
  },
)


###
### GreedyPLR experiments.
###

run_command(
  name="combine-amzn-greedyplr",
  run="python3 combine_raw.py",
  deps=[":amzn-greedyplr"],
)

run_experiment_group(
  name="amzn-greedyplr",
  run="./run.sh",
  experiments=[
    # e.g. amzn_greedyplr-pg_llsm-64B-a-zipfian-16
    ExperimentInstance(
      name="amzn_greedyplr-pg_llsm-{}-{}-zipfian-{}".format(config["name"], workload, threads),
      options={
        **COMMON_OPTIONS,
        **process_config("pg_llsm", config, AMZN_DATASET, workload=workload),
        "db": "pg_llsm",
        "checkpoint_name": "ycsb-amzn_greedyplr-pg_llsm-{}".format(config["name"]),
        "custom_dataset": AMZN_DATASET["path"],
        "threads": threads,
        "gen_template": "workloads/{}.yml".format(workload),
        "gen_distribution": "zipfian",
        "pg_use_pgm_builder": False,
      },
    )
    for config, workload, threads in product(CONFIGS, WORKLOADS, THREADS)
  ],
  deps=[
    # e.g. :preload-amzn-pg_llsm-64B
    ":preload-amzn_greedyplr-pg_llsm-{}".format(config["name"])
    for config in CONFIGS
  ],
)

group(
  name="greedyplr-preload",
  deps=[
    ":preload-{}_greedyplr-pg_llsm-{}".format(dataset["name"], config["name"])
    for dataset, config in product(CUSTOM_DATASETS, CONFIGS)
  ] + [
    ":preload-synth_greedyplr-pg_llsm-{}".format(config["name"])
    for config in CONFIGS
  ],
)

for dataset, config in product(CUSTOM_DATASETS, CONFIGS):
  run_command(
    name="preload-{}_greedyplr-pg_llsm-{}".format(dataset["name"], config["name"]),
    run="./preload.sh" ,
    options={
      **COMMON_OPTIONS,
      **process_config("pg_llsm", config, dataset),
      "db": "pg_llsm",
      "checkpoint_name": "ycsb-{}_greedyplr-pg_llsm-{}".format(dataset["name"], config["name"]),
      "custom_dataset": dataset["path"],
      "threads": 1,
      "gen_template": "workloads/setup.yml",
      # NOTE: This overrides any previously set value.
      "gen_num_requests": NUM_PRELOAD_UPDATES,
      "pg_use_pgm_builder": False,
    },
  )

for config in CONFIGS:
  run_command(
    name="preload-synth_greedyplr-pg_llsm-{}".format(config["name"]),
    run="./preload.sh" ,
    options={
      **COMMON_OPTIONS,
      **process_config("pg_llsm", config, SYNTH_DATASET),
      "db": "pg_llsm",
      "checkpoint_name": "ycsb-synth_greedyplr-pg_llsm-{}".format(config["name"]),
      "threads": 1,
      "gen_template": "workloads/setup.yml",
      # NOTE: This overrides any previously set value.
      "gen_num_requests": NUM_PRELOAD_UPDATES,
      "pg_use_pgm_builder": False,
    },
  )


###
### Prefetching experiments.
###
### NOTE: We only use the 16 thread results in the revision letter. To have
###       these experiments complete faster, change `THREADS` below to `[16]`.
###

PREFETCH_COMMON = {
  **COMMON_OPTIONS,
  **process_config("pg_llsm", CONFIG_1024B, AMZN_DATASET, workload="scan_only"),
  "db": "pg_llsm",
  "custom_dataset": AMZN_DATASET["path"],
  "gen_template": "workloads/scan_only.yml",
  "gen_distribution": "uniform",
}

PREFETCH_BG_THREADS = [32, 64, 128, 256]

run_command(
  name="combine-prefetch-amzn-1024B",
  run="python3 combine_raw.py",
  args=["--for-prefetch"],
  deps=[":prefetch-amzn-1024B"],
)

run_experiment_group(
  name="prefetch-amzn-1024B",
  run="./run.sh",
  experiments=[
    ExperimentInstance(
      name="prefetch-amzn-1024B-base-{}".format(threads),
      options={
        **PREFETCH_COMMON,
        "checkpoint_name": "ycsb-amzn-pg_llsm_nogrp-1024B",
        "threads": threads,
        "bg_threads": 4,  # Same as original experiments.
        # Disable page grouping.
        "pg_use_segments": False,
        # Disable prefetching.
        "use_experimental_scan_prefetching": False,
      },
    )
    for threads in THREADS
  ] + [
    ExperimentInstance(
      name="prefetch-amzn-1024B-prefetch-{}-{}".format(threads, bg_threads),
      options={
        **PREFETCH_COMMON,
        "checkpoint_name": "ycsb-amzn-pg_llsm_nogrp-1024B",
        "threads": threads,
        "bg_threads": bg_threads,
        # Disable page grouping.
        "pg_use_segments": False,
        # Enable prefetching.
        "use_experimental_scan_prefetching": True,
      },
    )
    for threads, bg_threads in product(THREADS, PREFETCH_BG_THREADS)
  ] + [
    ExperimentInstance(
      name="prefetch-amzn-1024B-grouping-{}".format(threads),
      options={
        **PREFETCH_COMMON,
        "checkpoint_name": "ycsb-amzn-pg_llsm-1024B",
        "threads": threads,
        "bg_threads": 4,  # Same as original experiments.
        # Enable page grouping.
        "pg_use_segments": True,
        # Disable prefetching.
        "use_experimental_scan_prefetching": False,
      },
    )
    for threads in THREADS
  ] + [
    ExperimentInstance(
      name="prefetch-amzn-1024B-grouping_prefetch-{}-{}".format(threads, bg_threads),
      options={
        **PREFETCH_COMMON,
        "checkpoint_name": "ycsb-amzn-pg_llsm-1024B",
        "threads": threads,
        "bg_threads": bg_threads,
        # Enable page grouping.
        "pg_use_segments": True,
        # Enable prefetching.
        "use_experimental_scan_prefetching": True,
      },
    )
    for threads, bg_threads in product(THREADS, PREFETCH_BG_THREADS)
  ],
  deps=[
    ":preload-amzn-pg_llsm_nogrp-1024B",
    ":preload-amzn-pg_llsm-1024B",
  ],
)

group(
  name="prefetch-vary",
  deps=[
    ":prefetch-amzn-1024B-prefetch-{}-{}".format(threads, bg_threads)
    for threads, bg_threads in product(THREADS, PREFETCH_BG_THREADS)
  ] + [
    ":prefetch-amzn-1024B-grouping_prefetch-{}-{}".format(threads, bg_threads)
    for threads, bg_threads in product(THREADS, PREFETCH_BG_THREADS)
  ],
)
